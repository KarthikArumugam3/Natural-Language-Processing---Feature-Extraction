{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing the text file\n",
    "with open('One.txt') as mytext:\n",
    "    a = mytext.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a story about dogs\\nour canine pets\\nDogs are furry animals\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a story about dogs\n",
      "our canine pets\n",
      "Dogs are furry animals\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('One.txt') as mytext:\n",
    "    a = mytext.readlines() # readlines returns a list of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a story about dogs\\n',\n",
       " 'our canine pets\\n',\n",
       " 'Dogs are furry animals\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a # readlines returns a list of lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, keep in mind, for really large text files, you may want to avoid actually displaying the output.\n",
    "\n",
    "It's not a big deal for Python to read in a large text file like this if you have enough RAM, but it may crash Jupiter If you're trying to print out an entire novel into this output cell, it'll just take forever to actually render."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-de40429eadb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Reading in each word seperately\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Reading in each file seperately \n",
    "a.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('One.txt') as mytext:\n",
    "    a = mytext.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'story',\n",
       " 'about',\n",
       " 'dogs',\n",
       " 'our',\n",
       " 'canine',\n",
       " 'pets',\n",
       " 'dogs',\n",
       " 'are',\n",
       " 'furry',\n",
       " 'animals']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we exactly want when we are thinking of the Bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Two.txt') as mytext2:\n",
    "    b = mytext2.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'story',\n",
       " 'is',\n",
       " 'about',\n",
       " 'surfing',\n",
       " 'catching',\n",
       " 'waves',\n",
       " 'is',\n",
       " 'fun',\n",
       " 'surfing',\n",
       " 'is',\n",
       " 'a',\n",
       " 'popular',\n",
       " 'water',\n",
       " 'sport']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's begin by building out a vocabulary.\n",
    "\n",
    "* And this idea of a vocabulary is extremely important in natural language processing.\n",
    "------\n",
    "1. We have to imagine that with all the documents available for us, there's going to be kind of this limited vocabulary available.\n",
    "\n",
    "2. So, for example, if you're dealing with something like the novel Moby Dick, there's only a certain amount of words that are used throughout that entire novel.\n",
    "\n",
    "3. And we can think of that array of unique words as the vocabulary for that particular document or that particular text.\n",
    "\n",
    "4. So we want to build a vocabulary for both one.txt and two.txt. So there are some words that are similar between the one.text and two.text and there's some words that are unique to each document.\n",
    "\n",
    "So this where we want to move from just count vectorization to being able to compare the frequency of certain words b/w the two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So let's begin the process by getting all the unique words across all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'animals',\n",
       " 'are',\n",
       " 'canine',\n",
       " 'dogs',\n",
       " 'furry',\n",
       " 'is',\n",
       " 'our',\n",
       " 'pets',\n",
       " 'story',\n",
       " 'this'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening one.txt and getting all the unique words\n",
    "with open('One.txt') as text1:\n",
    "    words_1 = text1.read().lower().split()\n",
    "    \n",
    "    #unique words\n",
    "    uni_words_1 = set(words_1)\n",
    "\n",
    "uni_words_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above we can see the unique words from all sentences/dcouments from one.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'catching',\n",
       " 'fun',\n",
       " 'is',\n",
       " 'popular',\n",
       " 'sport',\n",
       " 'story',\n",
       " 'surfing',\n",
       " 'this',\n",
       " 'water',\n",
       " 'waves'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening two.txt and  getting the unique words\n",
    "with open('Two.txt') as text2:\n",
    "    words_2 = text2.read().lower().split()\n",
    "    \n",
    "    #unique words\n",
    "    uni_words_2 = set(words_2)\n",
    "    \n",
    "uni_words_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above we can see the unique words from the allsentences/dcouments from two.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'animals',\n",
       " 'are',\n",
       " 'canine',\n",
       " 'dogs',\n",
       " 'furry',\n",
       " 'is',\n",
       " 'our',\n",
       " 'pets',\n",
       " 'story',\n",
       " 'this'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting unique words across all the documents i.e. one.txt + two.txt\n",
    "all_uni_words = set()\n",
    "all_uni_words.update(uni_words_1)\n",
    "all_uni_words # now has unique words from document 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'animals',\n",
       " 'are',\n",
       " 'canine',\n",
       " 'catching',\n",
       " 'dogs',\n",
       " 'fun',\n",
       " 'furry',\n",
       " 'is',\n",
       " 'our',\n",
       " 'pets',\n",
       " 'popular',\n",
       " 'sport',\n",
       " 'story',\n",
       " 'surfing',\n",
       " 'this',\n",
       " 'water',\n",
       " 'waves'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_uni_words.update(uni_words_2)\n",
    "all_uni_words # has all unique words from both one.txt and two.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning a number of each of these words\n",
    "full_vocab = dict()\n",
    "i = 0\n",
    "\n",
    "for word in all_uni_words:\n",
    "    full_vocab[word] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 0,\n",
       " 'pets': 1,\n",
       " 'story': 2,\n",
       " 'are': 3,\n",
       " 'animals': 4,\n",
       " 'about': 5,\n",
       " 'surfing': 6,\n",
       " 'furry': 7,\n",
       " 'a': 8,\n",
       " 'is': 9,\n",
       " 'water': 10,\n",
       " 'waves': 11,\n",
       " 'our': 12,\n",
       " 'popular': 13,\n",
       " 'fun': 14,\n",
       " 'catching': 15,\n",
       " 'canine': 16,\n",
       " 'dogs': 17,\n",
       " 'sport': 18}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_freq = [0]*len(full_vocab)\n",
    "two_freq = [0]*len(full_vocab)\n",
    "all_words = ['']*len(full_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here are kind of my first frequency counts for all the words in my actual documents, and it's the same thing for two frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what I'm going to do is go through those documents and every time I encounter a word, I'll look it up in my full_vocab, take a note of the index position such as zero.\n",
    "\n",
    "And then what's going to happen is zero here essentially stands for the word \"this\" - 0th word in full_vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'story',\n",
       " 'about',\n",
       " 'dogs',\n",
       " 'our',\n",
       " 'canine',\n",
       " 'pets',\n",
       " 'dogs',\n",
       " 'are',\n",
       " 'furry',\n",
       " 'animals']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('One.txt') as f:\n",
    "    one_t = f.read().lower().split()\n",
    "    \n",
    "one_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is happening is :\n",
    "1. First we look through \"one_t\" that contains all the words from the documents/sentences from one.txt\n",
    "2. Then from the \"full_vocab\" that has all the unique words from boths text file we'll get the index of that particular word\n",
    "3. And finally in the one_freq list that has counts for the unique words from one.text we will add frequency each time we see the word in \"one_t\" list at the index we got from \"full_vocab\" dict.\n",
    "\n",
    "* Ex: - if we find the word dogs in \"one_t\" and in \"full_vocab\" dogs is at index 17 th index, then we will add 1 at 17th index in the \"one_freq\" list and continue the process for all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in one_t:\n",
    "    word_index = full_vocab[word]\n",
    "    one_freq[word_index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 2, 0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'story',\n",
       " 'is',\n",
       " 'about',\n",
       " 'surfing',\n",
       " 'catching',\n",
       " 'waves',\n",
       " 'is',\n",
       " 'fun',\n",
       " 'surfing',\n",
       " 'is',\n",
       " 'a',\n",
       " 'popular',\n",
       " 'water',\n",
       " 'sport']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for two.txt\n",
    "with open('Two.txt') as s:\n",
    "    two_t = s.read().lower().split()\n",
    "    \n",
    "two_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in two_t:\n",
    "    word_indexes = full_vocab[word]\n",
    "    two_freq[word_indexes] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 0, 1, 2, 0, 1, 3, 1, 1, 0, 1, 1, 1, 0, 0, 1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we'll fill out the all_words with the unique words in the full-vocab dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in full_vocab:\n",
    "    word_index = full_vocab[word]\n",
    "    all_words[word_index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'pets',\n",
       " 'story',\n",
       " 'are',\n",
       " 'animals',\n",
       " 'about',\n",
       " 'surfing',\n",
       " 'furry',\n",
       " 'a',\n",
       " 'is',\n",
       " 'water',\n",
       " 'waves',\n",
       " 'our',\n",
       " 'popular',\n",
       " 'fun',\n",
       " 'catching',\n",
       " 'canine',\n",
       " 'dogs',\n",
       " 'sport']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this list we have all the unique words from the full_vocab dict with their indexes matching up with full_vocab and one_freq and two_freq, as we just did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Finally we can build a nice dataframe to organise this info from all_words, one_freq and two_freq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>pets</th>\n",
       "      <th>story</th>\n",
       "      <th>are</th>\n",
       "      <th>animals</th>\n",
       "      <th>about</th>\n",
       "      <th>surfing</th>\n",
       "      <th>furry</th>\n",
       "      <th>a</th>\n",
       "      <th>is</th>\n",
       "      <th>water</th>\n",
       "      <th>waves</th>\n",
       "      <th>our</th>\n",
       "      <th>popular</th>\n",
       "      <th>fun</th>\n",
       "      <th>catching</th>\n",
       "      <th>canine</th>\n",
       "      <th>dogs</th>\n",
       "      <th>sport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   this  pets  story  are  animals  about  surfing  furry  a  is  water  \\\n",
       "0     1     1      1    1        1      1        0      1  1   1      0   \n",
       "1     1     0      1    0        0      1        2      0  1   3      1   \n",
       "\n",
       "   waves  our  popular  fun  catching  canine  dogs  sport  \n",
       "0      0    1        0    0         0       1     2      0  \n",
       "1      1    0        1    1         1       0     0      1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOW - bag of words\n",
    "bow = pd.DataFrame(data=[one_freq, two_freq],columns=all_words)\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what is this bow - bag of words\n",
    "This is known as a bag of words model, which is a frequency count of all the words in the documents.\n",
    "\n",
    "And just for visualization purposes here, I've shown you all the words and then the frequency counts.\n",
    "\n",
    "The first row shows the no of times all the words from all_words show up in the sentences/documents in one.txt file and the same for second row which represents the second.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So this is how Scikit learn directly converts text into this frquency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points:-\n",
    "We can now imagine that more similar these counts are between two documents could mean the documenst themselves are more common i.e. they could be talking about the same thing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before using Scikit-learn.\n",
    "\n",
    "1. Right now we have this bag of words model, but by itself, these may not actually be too helpful until we consider term frequencies, as well as how often these individual words appear in the documents.\n",
    "\n",
    "2. If you're dealing with a bunch of documents that all happen to be part of, for instance, the same general category of a topic, for example, you're dealing with just text words in the sports section, of a newspaper and you're trying to classify things between soccer, baseball or basketball, etc There may be certain words that appear in all the documents, for example, run or running, that you do in all those sports. \n",
    "\n",
    "3. And we're going to have to start considering is not just is this word common in the English language, which is the idea behind stop words. So words such as \"a\",\"the\",\"and\" are those are very common across the entire English language. It could be a good idea just to remove them across all documents.\n",
    "\n",
    "4. The other thing we want to consider are for the particular scope of all our documents or any particular words showing up a lot across all the documents versus just a few documents. For example, you're not going to see the word \"soccer\" show up across all sports documents as it is specific to one sport only and will only show up soccer related articles.\n",
    "\n",
    "5. So the idea of TF-IDF term frequency inverse document frequency is going to help to alleviate/overcome those issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
